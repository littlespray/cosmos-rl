redis = "12800"

[train]
resume = false
epoch = 1
output_dir = "./outputs/qwen3-vl-30b-a3b-fsdp8-sft"
epsilon = 1e-6
optm_name = "Muon"
optm_lr = 1e-6
optm_impl = "fused"
optm_weight_decay = 0.01
optm_betas = [ 0.9, 0.999,]
optm_warmup_steps = 1
optm_grad_norm_clip = 1.0
async_tp_enabled = false
compile = false
param_dtype = "bfloat16"
fsdp_reduce_dtype = "float32"
master_dtype = "bfloat16"
fsdp_offload = false
fsdp_reshard_after_forward = "default"
train_batch_per_replica = 8
sync_weight_interval = 1
deterministic = false
max_num_steps = 5

[policy]
model_name_or_path = "Qwen/Qwen3-VL-30B-A3B-Instruct"
model_max_length = 16384
model_gradient_checkpointing = true

[logging]
logger = ['console', 'wandb']
project_name = "enscale-debug"
experiment_name = "qwen3-vl-30b-a3b-sft"

[train.train_policy]
type = "sft"
dataset.name = "HuggingFaceH4/llava-instruct-mix-vsft"
dataset.subset = ""
dataset.split = "train"
dataset.test_size = 1
enable_dataset_cache = false
dataloader_num_workers = 4
dataloader_prefetch_factor = 4
conversation_column_name = ""
mini_batch = 1
dataloader_shuffle = true

[train.ckpt]
enable_checkpoint = true
save_freq = 1
save_mode = "async"
max_keep = 5
upload_hf = true
hf_repo_name = "debug"

[policy.parallelism]
n_init_replicas = 1
tp_size = 1
cp_size = 1
dp_shard_size = 8
pp_size = 1
dp_replicate_size = 1


[policy.enscale]
enable = true
enscale_model_name = "shangkuns/dinov3-vith16plus-pretrain-lvd1689m"
scale_idx = [6, -1]
injected_layers_idx = "all"
num_heads = 8
ffn_multiplier = 2